{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989ffbc3-8c13-4748-ae44-d2dbd4a2c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bibliothèques importées.\n",
      "✅ Dataset chargé. Il contient 2830743 lignes et 79 colonnes.\n"
     ]
    }
   ],
   "source": [
    "# --- ÉTAPE 1 : IMPORTS ET CHARGEMENT ---\n",
    "\n",
    "# --- 1.1 Chargement des packages Python nécessaires ---\n",
    "# Pour la manipulation des données (tableaux, calculs)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pour interagir avec le système de fichiers (trouver nos CSV)\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Pour la modélisation et l'évaluation en Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Pour gérer le déséquilibre des classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Pour sauvegarder notre modèle final et les transformateurs\n",
    "import joblib\n",
    "\n",
    "# Utilitaire pour un affichage plus propre\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Bibliothèques importées.\")\n",
    "\n",
    "# --- 1.2 Chargement du dataset CIC-IDS2017 ---\n",
    "# On spécifie le chemin vers le dossier contenant les fichiers\n",
    "dataset_path = 'C:/Users/MTechno/Downloads/MachineLearningCVE'\n",
    "# On utilise glob pour créer une liste de tous les fichiers .csv dans ce dossier\n",
    "csv_files = glob.glob(os.path.join(dataset_path, \"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"❌ ERREUR : Aucun fichier CSV trouvé dans le dossier 'data/'. Veuillez vérifier le chemin.\")\n",
    "else:\n",
    "    # On charge chaque fichier et on les assemble en un seul grand tableau (DataFrame)\n",
    "    df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "    print(f\"✅ Dataset chargé. Il contient {df.shape[0]} lignes et {df.shape[1]} colonnes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ee9802-7030-4fee-91f2-a38a4b505de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions avant nettoyage : (2830743, 79)\n",
      "Colonnes redondantes supprimées : ['Bwd PSH Flags', 'Bwd URG Flags', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\n",
      "\n",
      "✅ Nettoyage terminé. Dimensions finales : (2520798, 71)\n"
     ]
    }
   ],
   "source": [
    "# --- ÉTAPE 2 : EXPLORATION ET NETTOYAGE ---\n",
    "\n",
    "print(f\"Dimensions avant nettoyage : {df.shape}\")\n",
    "\n",
    "# 2.1 Nettoyage des noms de colonnes (supprime les espaces comme ' Label' -> 'Label')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# 2.2 Remplacement des valeurs infinies (résultant de divisions par zéro) par NaN (Not a Number)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# 2.3 Suppression des lignes contenant des valeurs manquantes (NaN)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 2.4 Suppression des lignes entièrement dupliquées\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# 2.5 Détection et suppression automatique des colonnes redondantes\n",
    "# Une colonne est redondante si elle a la même valeur pour toutes les lignes (variance nulle)\n",
    "cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "if cols_to_drop:\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"Colonnes redondantes supprimées : {cols_to_drop}\")\n",
    "\n",
    "print(f\"\\n✅ Nettoyage terminé. Dimensions finales : {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b0e908-7731-432f-9125-04ec61013028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle distribution des classes après regroupement :\n",
      "Label\n",
      "BENIGN                  2095057\n",
      "DoS                      193745\n",
      "DDoS                     128014\n",
      "PortScan                  90694\n",
      "Brute Force                9150\n",
      "Web Attack                 2143\n",
      "Bot                        1948\n",
      "Infiltration/Exploit         47\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ Labels regroupés et encodés.\n"
     ]
    }
   ],
   "source": [
    "# --- ÉTAPE 3 : PRÉTRAITEMENT DES LABELS ---\n",
    "\n",
    "# 3.1 Dictionnaire pour regrouper les attaques similaires\n",
    "attack_mapping = {\n",
    "    'BENIGN': 'BENIGN', 'DDoS': 'DDoS', 'PortScan': 'PortScan', 'Bot': 'Bot',\n",
    "    'DoS GoldenEye': 'DoS', 'DoS Hulk': 'DoS', 'DoS Slowhttptest': 'DoS', 'DoS slowloris': 'DoS',\n",
    "    'FTP-Patator': 'Brute Force', 'SSH-Patator': 'Brute Force',\n",
    "    'Web Attack � Brute Force': 'Web Attack', 'Web Attack � XSS': 'Web Attack', 'Web Attack � Sql Injection': 'Web Attack',\n",
    "    'Infiltration': 'Infiltration/Exploit', 'Heartbleed': 'Infiltration/Exploit'\n",
    "}\n",
    "\n",
    "# 3.2 Application du regroupement\n",
    "df['Label'] = df['Label'].map(attack_mapping)\n",
    "print(\"Nouvelle distribution des classes après regroupement :\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# 3.3 Séparation des données en caractéristiques (X) et cible (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y_temp = df['Label']\n",
    "\n",
    "# 3.4 Encodage de la cible en valeurs numériques (ex: BENIGN -> 0, Bot -> 1, etc.)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_temp)\n",
    "\n",
    "print(\"\\n✅ Labels regroupés et encodés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f17c01-8a00-4e03-b252-9a02d192331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données divisées en ensembles d'entraînement et de test.\n",
      "Taille de l'entraînement : 1764558 échantillons.\n",
      "Taille du test : 756240 échantillons.\n"
     ]
    }
   ],
   "source": [
    "# --- ÉTAPE 4 : SÉPARATION DES DONNÉES ---\n",
    "\n",
    "# On divise le dataset : 70% pour l'entraînement, 30% pour le test.\n",
    "# 'stratify=y' garantit que la proportion de chaque attaque est la même dans les deux ensembles.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Données divisées en ensembles d'entraînement et de test.\")\n",
    "print(f\"Taille de l'entraînement : {X_train.shape[0]} échantillons.\")\n",
    "print(f\"Taille du test : {X_test.shape[0]} échantillons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f00e922-eddd-4740-92fe-2667e831be51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Données normalisées.\n",
      "\n",
      "Distribution des classes d'entraînement AVANT SMOTE :\n",
      "0    1466539\n",
      "4     135621\n",
      "3      89610\n",
      "6      63486\n",
      "2       6405\n",
      "7       1500\n",
      "1       1364\n",
      "5         33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution des classes d'entraînement APRES SMOTE :\n",
      "4    1466539\n",
      "6    1466539\n",
      "0    1466539\n",
      "3    1466539\n",
      "2    1466539\n",
      "7    1466539\n",
      "1    1466539\n",
      "5    1466539\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. Rééchantillonnage SMOTE appliqué.\n",
      "\n",
      "3. Réduction de dimensionnalité avec PCA appliquée.\n",
      "Nombre de caractéristiques réduit à 1.\n"
     ]
    }
   ],
   "source": [
    "# --- ÉTAPE 5 : PRÉTRAITEMENT AVANCÉ ET RÉDUCTION DE DIMENSIONNALITÉ ---\n",
    "\n",
    "# 5.1 Normalisation des features\n",
    "# On apprend la mise à l'échelle sur X_train et on l'applique aux deux ensembles.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"1. Données normalisées.\")\n",
    "\n",
    "# 5.2 Application de SMOTE (uniquement sur les données d'entraînement)\n",
    "print(\"\\nDistribution des classes d'entraînement AVANT SMOTE :\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nDistribution des classes d'entraînement APRES SMOTE :\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "print(\"\\n2. Rééchantillonnage SMOTE appliqué.\")\n",
    "\n",
    "# 5.3 Réduction de la dimensionnalité avec PCA\n",
    "# On apprend la PCA sur les données d'entraînement (équilibrées par SMOTE) et on l'applique aux deux ensembles.\n",
    "pca = PCA(n_components=0.95, random_state=42) # Conserver 95% de la variance\n",
    "X_train_pca = pca.fit_transform(X_train_smote)\n",
    "X_test_pca = pca.transform(X_test_scaled) # On utilise le PCA appris sur le train\n",
    "print(\"\\n3. Réduction de dimensionnalité avec PCA appliquée.\")\n",
    "print(f\"Nombre de caractéristiques réduit à {X_train_pca.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f2971-d185-41d8-8848-b5efd34c9a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du RandomForestClassifier...\n"
     ]
    }
   ],
   "source": [
    "# --- ÉTAPE 6 : MODÉLISATION ET ÉVALUATION ---\n",
    "\n",
    "# 6.1 Entraînement du modèle\n",
    "print(\"\\nEntraînement du RandomForestClassifier...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_pca, y_train_smote)\n",
    "print(\"Modèle entraîné.\")\n",
    "\n",
    "# 6.2 Évaluation du modèle sur l'ensemble de test\n",
    "print(\"\\n--- Évaluation du modèle sur les données de test ---\")\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# On décode les prédictions pour un rapport lisible\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "y_test_decoded = le.inverse_transform(y_test)\n",
    "\n",
    "print(classification_report(y_test_decoded, y_pred_decoded, digits=4))\n",
    "print(\"\\n✅ Modélisation et Évaluation terminées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e5bbc-ad5a-487d-b48e-6edc861faa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ÉTAPE 7 : SAUVEGARDE FINALE DES ARTEFACTS ---\n",
    "\n",
    "# S'assurer que la bibliothèque pour la sauvegarde est importée\n",
    "import joblib\n",
    "\n",
    "print(\"--- Début de la sauvegarde des artefacts du pipeline ---\")\n",
    "\n",
    "# 1. Définir les noms des fichiers de sauvegarde\n",
    "encoder_filename = 'final_label_encoder.joblib'\n",
    "scaler_filename = 'final_scaler.joblib'\n",
    "pca_filename = 'final_pca.joblib'\n",
    "model_filename = 'final_model.joblib'\n",
    "\n",
    "# 2. Sauvegarder chaque objet dans son propre fichier\n",
    "#    - L'encodeur de labels (pour convertir 0, 1, 2... en 'BENIGN', 'DDoS', etc.)\n",
    "joblib.dump(le, encoder_filename)\n",
    "print(f\"✅ Label Encoder sauvegardé sous : '{encoder_filename}'\")\n",
    "\n",
    "#    - Le normalisateur (pour mettre à l'échelle les nouvelles données)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"✅ Scaler sauvegardé sous : '{scaler_filename}'\")\n",
    "\n",
    "#    - Le réducteur de dimension (pour appliquer la PCA aux nouvelles données)\n",
    "joblib.dump(pca, pca_filename)\n",
    "print(f\"✅ Objet PCA sauvegardé sous : '{pca_filename}'\")\n",
    "\n",
    "#    - Le modèle RandomForest entraîné lui-même\n",
    "joblib.dump(model, model_filename)\n",
    "print(f\"✅ Modèle sauvegardé sous : '{model_filename}'\")\n",
    "\n",
    "print(\"\\n--- Sauvegarde terminée ---\")\n",
    "print(\"Vous avez maintenant les 4 fichiers nécessaires pour utiliser votre modèle dans un autre script.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
